{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIXuqZpSuX9CravwFhEdjT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krixhnaprasad/DefBotAI/blob/main/DefBotAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w3iJ4FieY90"
      },
      "outputs": [],
      "source": [
        "pip install pymupdf pillow pytesseract\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Optional: Set tesseract path manually if needed\n",
        "# pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
        "\n",
        "def enhance_image_for_ocr(image):\n",
        "    \"\"\"\n",
        "    Enhance image for improved OCR accuracy:\n",
        "    - Convert to grayscale\n",
        "    - Apply adaptive thresholding\n",
        "    - Increase contrast\n",
        "    \"\"\"\n",
        "    gray = image.convert(\"L\")\n",
        "    gray = gray.filter(ImageFilter.MedianFilter())\n",
        "    enhanced = ImageEnhance.Contrast(gray).enhance(2.5)\n",
        "    thresholded = ImageOps.autocontrast(ImageOps.invert(enhanced))\n",
        "    return thresholded\n",
        "\n",
        "def extract_text_and_ocr_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract full page text and OCR text from images in the PDF\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    combined_text = \"\"\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        text = page.get_text()\n",
        "        combined_text += f\"\\n[Page {page_num + 1} Text]\\n{text.strip()}\\n\"\n",
        "\n",
        "        images = page.get_images(full=True)\n",
        "        for img_index, img in enumerate(images):\n",
        "            try:\n",
        "                xref = img[0]\n",
        "                base_image = doc.extract_image(xref)\n",
        "                image_bytes = base_image[\"image\"]\n",
        "                image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "                enhanced_image = enhance_image_for_ocr(image)\n",
        "                ocr_text = pytesseract.image_to_string(enhanced_image)\n",
        "                combined_text += f\"\\n[Page {page_num + 1} Image {img_index + 1} OCR Text]\\n{ocr_text.strip()}\\n\"\n",
        "            except Exception as e:\n",
        "                combined_text += f\"\\n[Image OCR Failed on Page {page_num + 1} Image {img_index + 1}]: {str(e)}\\n\"\n",
        "\n",
        "    doc.close()\n",
        "    return combined_text.replace('\"', \"'\").replace('\\n', ' ').strip()\n",
        "\n",
        "def create_modelfile(pdf_path, output_file=\"Modelfile.txt\", base_model=\"llama3\"):\n",
        "    \"\"\"\n",
        "    Generate a complete Modelfile.txt from a PDF using both text and OCR image data\n",
        "    \"\"\"\n",
        "    print(\"üîç Extracting text and OCR content from PDF...\")\n",
        "    context = extract_text_and_ocr_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"üìù Writing Modelfile content...\")\n",
        "    modelfile_content = f\"\"\"FROM {base_model}\n",
        "\n",
        "SYSTEM You are a highly knowledgeable and helpful assistant. The following content is extracted from a PDF document and may include both typed and scanned image-based data. Use it to answer user questions accurately and contextually.\n",
        "\n",
        "SYSTEM {context}\n",
        "\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER num_ctx 4096\n",
        "\n",
        "TEMPLATE \\\"\\\"\\\"{{{{ if .System }}}}System: {{{{ .System }}}}{{{{ end }}}}\n",
        "{{{{ if .Prompt }}}}User: {{{{ .Prompt }}}}{{{{ end }}}}\n",
        "Assistant:\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(modelfile_content)\n",
        "\n",
        "    print(f\"‚úÖ Modelfile created at: {os.path.abspath(output_file)}\")\n",
        "\n",
        "# --------------------------\n",
        "# üìå Run Example\n",
        "# --------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_input_path = input(\"üìÇ Enter the path to your PDF file: \").strip()\n",
        "    create_modelfile(pdf_input_path)\n"
      ],
      "metadata": {
        "id": "oGBEK91Qepx5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}